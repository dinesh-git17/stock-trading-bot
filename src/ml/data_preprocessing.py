import pandas as pd
import numpy as np
from sklearn.preprocessing import MinMaxScaler, StandardScaler
from sklearn.model_selection import train_test_split
from dotenv import load_dotenv
import os
import logging
import psycopg2
from rich.console import Console
from rich.progress import Progress, SpinnerColumn, TextColumn
import warnings

# Load environment variables
load_dotenv()

# Setup logging
logging.basicConfig(
    filename="data/logs/data_preprocessing.log",
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
)

console = Console()

# Suppress any warnings
warnings.filterwarnings("ignore", category=FutureWarning)

# Database Configuration
DB_NAME = os.getenv("DB_NAME")
DB_USER = os.getenv("DB_USER")
DB_PASSWORD = os.getenv("DB_PASSWORD")
DB_HOST = os.getenv("DB_HOST")
DB_PORT = os.getenv("DB_PORT")


# Database connection function
def connect_db():
    """Connects to the PostgreSQL database."""
    try:
        conn = psycopg2.connect(
            dbname=DB_NAME,
            user=DB_USER,
            password=DB_PASSWORD,
            host=DB_HOST,
            port=DB_PORT,
        )
        return conn
    except Exception as e:
        console.print(f"[bold red]❌ Database connection error:[/bold red] {e}")
        logging.error(f"Database connection error: {e}")
        return None


# Step 1: Scaling Data
def scale_data(df, method="minmax"):
    """Scales stock data using either MinMaxScaler or StandardScaler."""
    if method == "minmax":
        scaler = MinMaxScaler()
    elif method == "standard":
        scaler = StandardScaler()
    else:
        raise ValueError("Scaling method must be 'minmax' or 'standard'")

    # Exclude non-numeric columns
    columns_to_scale = df.select_dtypes(include=[np.number]).columns
    df[columns_to_scale] = scaler.fit_transform(df[columns_to_scale])
    return df


# Step 2: Feature Engineering (Rolling Averages, RSI, etc.)
def feature_engineering(df):
    """Perform feature engineering, e.g., adding rolling averages."""
    # Add 3-day rolling average for Close price
    df["rolling_3d_avg"] = df["close"].rolling(window=3).mean()

    # Add 5-day rolling average for Close price (you can adjust as needed)
    df["rolling_5d_avg"] = df["close"].rolling(window=5).mean()

    # Example of adding other technical indicators (RSI calculation with check for window size)
    def calculate_rsi(series, window=7):  # Using a smaller window for RSI
        """Calculates RSI with a check for enough data points."""
        if len(series) < window:
            return np.nan  # Return NaN if not enough data for the window
        delta = series.diff()
        gain = delta.where(delta > 0, 0)
        loss = -delta.where(delta < 0, 0)
        avg_gain = gain.rolling(window=window, min_periods=1).mean()
        avg_loss = loss.rolling(window=window, min_periods=1).mean()
        rs = avg_gain / avg_loss
        rsi = 100 - (100 / (1 + rs))
        return rsi.iloc[-1]  # Return the last RSI value

    df["rsi_7"] = (
        df["close"].rolling(window=7).apply(lambda x: calculate_rsi(x), raw=False)
    )

    # Fill NaN values generated by rolling calculations using forward fill or interpolation
    df.fillna(method="ffill", inplace=True)

    return df


def handle_missing_data(df):
    """Handles missing data by interpolation and forward fill."""
    df.ffill(inplace=True)  # Forward fill missing values
    df.interpolate(method="linear", inplace=True)  # Linear interpolation if needed
    return df


# Step 4: Create Time-Lagged Features
def create_time_lagged_features(df, lag=1):
    """Creates time-lagged features for sequential dependencies."""
    df_lagged = df.copy()

    # Lag the 'close' column by 1 day
    df_lagged["close_lag"] = df_lagged["close"].shift(lag)

    # Lag other features if needed (e.g., RSI, EMA)
    df_lagged["rsi_7_lag"] = df_lagged["rsi_7"].shift(lag)
    df_lagged["ema_20_lag"] = df_lagged["rolling_5d_avg"].shift(lag)

    # Drop NaN values generated by lagging
    df_lagged.fillna(method="ffill", inplace=True)  # Forward fill missing values

    return df_lagged


# Step 5: Split Data into Features and Target, Train-Test Split
def preprocess_data(ticker, scale_method="minmax", lag=1):
    """Preprocess the data by scaling, feature engineering, handling missing data, and splitting into train-test sets."""
    console.print(f"\n[bold yellow]Preprocessing data for {ticker}...[/bold yellow]")

    # Load the training data from the saved CSV
    file_path = f"data/training_data_files/training_data_{ticker}.csv"
    if not os.path.exists(file_path):
        logging.warning(f"Training data file for {ticker} does not exist!")
        return None

    df = pd.read_csv(file_path)

    # Step 1: Scale the data
    df = scale_data(df, method=scale_method)

    # Step 2: Perform feature engineering
    df = feature_engineering(df)

    # Step 3: Handle missing data
    df = handle_missing_data(df)

    # Step 4: Create time-lagged features
    df = create_time_lagged_features(df, lag=lag)

    # Step 5: Split into features (X) and target (y)
    X = df.drop(
        columns=["date", "close"]
    )  # Drop the 'date' and 'close' columns from features
    y = df["close"]  # The target is the closing price

    # Check if we have enough data to split
    if X.shape[0] == 0 or y.shape[0] == 0:
        logging.warning(f"Not enough data for {ticker} to perform train-test split.")
        return None

    # Train-test split (80% training, 20% testing)
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, shuffle=False
    )

    # Ensure the 'processed_data' directory exists before saving
    processed_folder = "data/processed_data/"
    os.makedirs(processed_folder, exist_ok=True)  # Ensure the folder exists
    processed_file = os.path.join(processed_folder, f"{ticker}_processed.csv")
    df.to_csv(processed_file, index=False)
    logging.info(f"Preprocessed data for {ticker} saved to {processed_file}")
    console.print(
        f"[bold green]✅ Data for {ticker} preprocessed and saved![/bold green]"
    )

    return X_train, X_test, y_train, y_test


# Step 6: Process Multiple Tickers
def process_all_tickers():
    """Preprocesses data for all tickers in the database."""
    # Fetch all tickers from the database
    conn = connect_db()
    if not conn:
        return
    cursor = conn.cursor()

    cursor.execute("SELECT DISTINCT ticker FROM stocks;")
    tickers = [row[0] for row in cursor.fetchall()]
    conn.close()

    with Progress(
        SpinnerColumn(),
        TextColumn("[bold yellow]Preprocessing data...[/bold yellow]"),
        console=console,
    ) as progress:
        task = progress.add_task("processing", total=len(tickers))

        for ticker in tickers:
            preprocess_data(ticker)
            progress.update(task, advance=1)

    console.print(
        "[bold green]✅ Data preprocessing completed for all tickers![/bold green]"
    )


if __name__ == "__main__":
    process_all_tickers()
